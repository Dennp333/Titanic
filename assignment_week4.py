# -*- coding: utf-8 -*-
"""Assignment_Week4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sh8mXH4Gbr_dsTpWsLboIJdl4Ur4_FdN

## MLBootcamp21 Week 4 Assignment: Gridsearch and CrossValidation
- This week we will focus on improving our existing models using these two techniques
- They are good approaches to improving your model's performance apart from feature engineering

## Q1. Use GridSearchCV and also explore other GridSearch techniques provided by scikit-learn to tune your hyperparameters and obtain the best model

- Try out Decision Trees, Random Forest, GradientBoost, AdaBoost and also XGBoost 
- Run the model on the features you have generated until now without tuning the parameters first to check the result on basic parameters
- Then apply GridSearchCV or other GridSearch techniques provided by scikit-learn to tune the hyperparameters and get results on the best model

## Keep the "random_state" number as 42 or anynumber of your choice and report that number for me to be able to reproduce the same results

- Report your performance on the test set after making the submission on kaggle. 

- ****Do not use some random existing notebook on Kaggle to get the best results as you will not learn anything that way and we will be able to easily know if that has been done. Do whatever you can****
"""

# Write your code from this cell
# It need not be in a single cell
import pandas as pd
import re
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# Feature engineering
df = pd.read_csv('./train.csv')
df['Mrs'] = df['Name'].apply(lambda x: 1 if 'Mrs.' in x else 0)
df['Sex_Category'] = df['Sex'].apply(lambda x: 1 if x == 'female' else 0)
ageScalar = StandardScaler()
fareScaler = StandardScaler()
df['Age_Normalized'] = ageScalar.fit_transform(np.array(df['Age']).reshape(-1, 1))
df['Fare_Normalized'] = fareScaler.fit_transform(np.array(df['Fare']).reshape(-1, 1))
# df.head()

# Drop features
drop_features = ['Name', 'Sex', 'Age', 'Ticket', 'Cabin', 'Fare', 'Embarked']
df.drop(drop_features, inplace=True, axis=1)
# df.head()

# Drop missing values
df.dropna(inplace = True)
# df.isnull().sum()

# Do the same for test dataset
df_test = pd.read_csv('./test.csv')
df_test['Mrs'] = df_test['Name'].apply(lambda x: 1 if 'Mrs.' in x else 0)
df_test['Sex_Category'] = df_test['Sex'].apply(lambda x: 1 if x == 'female' else 0)
ageScalar = StandardScaler()
fareScaler = StandardScaler()
df_test['Age_Normalized'] = ageScalar.fit_transform(np.array(df_test['Age']).reshape(-1, 1))
df_test['Fare_Normalized'] = fareScaler.fit_transform(np.array(df_test['Fare']).reshape(-1, 1))
drop_features = ['Name', 'Sex', 'Age', 'Ticket', 'Cabin', 'Fare', 'Embarked']
df_test.drop(drop_features, inplace=True, axis=1)
df_test.fillna(method = 'ffill', inplace = True)
# df.isnull().sum()
x_test = df_test.loc[:, 'Pclass':]
# x_test

# Get x and y train
x_train = df.loc[:, 'Pclass':]
y_train = df.Survived

# Decision Tree
model = DecisionTreeClassifier()
model.fit(x_train,y_train)
# result = 0.69377

# Random Forrest
model = RandomForestClassifier()
model.fit(x_train,y_train)
# result = 0.74641

# Gradient Boost
model = GradientBoostingClassifier()
model.fit(x_train, y_train)
# result = 0.76076

# XGBoost

# Adaboost
model = AdaBoostClassifier()
model.fit(x_train, y_train)
# result = 0.72488

# GridSearch CV with Gradient Boost
parameter_grid = {
    'criterion': ['friedman_mse', 'mse', 'mae'],
    'n_estimators': [100, 150, 200],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'max_leaf_nodes': [5, 10, None],
    'learning_rate': [0.1, 0.15, 0.2],
    'loss': ['deviance', 'exponential'],
    'warm_start': [True, False]
}
model = GridSearchCV(cv = 5, estimator = GradientBoostingClassifier(), 
                     param_grid = parameter_grid)
model.fit(x_train, y_train)

model.best_params_

# Gradient boost with tuned parameters
model_tuned = GradientBoostingClassifier(criterion = 'friedman_mse', max_features = 'auto', max_leaf_nodes = 10, n_estimators = 150,
                                         random_state = 42)
model_tuned.fit(x_train, y_train)
# result: 0.76555

# Try again
model_tuned = GradientBoostingClassifier(criterion = 'friedman_mse', max_features = 'auto', max_leaf_nodes = 10, n_estimators = 150,
                                         random_state = 42, learning_rate = 0.2, loss = 'exponential', subsample = 1, warm_start = True)
model_tuned.fit(x_train, y_train)
# result: 0.75598

# One more time
model_tuned = GradientBoostingClassifier(criterion = 'friedman_mse', max_features = 'log2', max_leaf_nodes = None, n_estimators = 150,
                                         random_state = 42, learning_rate = 0.15, loss = 'deviance', warm_start = False)
model_tuned.fit(x_train, y_train)
# result: 0.75358

# For submission. Name the chosen model as simply "model"
submission = model.predict(x_test)
df_submission = df_test[['PassengerId']].copy()
df_submission['Survived'] = submission
df_submission.to_csv('submission.csv')

# For tuned submissions. Name the chosen model as "model_tuned"
submission = model_tuned.predict(x_test)
df_submission = df_test[['PassengerId']].copy()
df_submission['Survived'] = submission
df_submission.to_csv('submission_tuned.csv')
